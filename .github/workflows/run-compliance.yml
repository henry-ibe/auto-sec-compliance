name: Run Compliance Scan

on:
  workflow_dispatch:
    inputs:
      instance_id:
        description: "EC2 InstanceId (optional; defaults to first SSM Online Linux)"
        required: false
      bucket:
        description: "S3 bucket to store reports (required)"
        required: true
      region:
        description: "AWS region"
        required: true
        default: "us-east-1"
      publish_dashboard:
        description: "Update CloudWatch dashboard after publishing metrics"
        type: boolean
        default: true

jobs:
  run:
    runs-on: ubuntu-24.04
    env:
      AWS_REGION: ${{ inputs.region }}
      AWS_DEFAULT_REGION: ${{ inputs.region }}
      BUCKET: ${{ inputs.bucket }}
      PHASE: demo

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Ensure jq (runner)
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Configure AWS via OIDC
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ vars.ROLE_ARN }}   # set repo variable ROLE_ARN to your role ARN
          aws-region: ${{ inputs.region }}
          audience: sts.amazonaws.com
          output-env-credentials: true

      - name: Resolve target instance (SSM Online or provided)
        id: resolve
        env:
          INSTANCE_ID_INPUT: ${{ inputs.instance_id }}
        run: |
          set -euo pipefail
          IID="${INSTANCE_ID_INPUT:-}"
          if [ -z "$IID" ] || [ "$IID" = "None" ]; then
            IID=$(aws ssm describe-instance-information \
              --filters "Key=PingStatus,Values=Online" "Key=PlatformType,Values=Linux" \
              --query 'InstanceInformationList[0].InstanceId' --output text || true)
          fi
          if [ -z "$IID" ] || [ "$IID" = "None" ]; then
            echo "No SSM Online instance found and no instance_id provided." >&2
            exit 1
          fi
          echo "iid=$IID" >> "$GITHUB_OUTPUT"

      - name: Build and stage remote script (2-pass: before & after)
        id: stage
        env:
          IID: ${{ steps.resolve.outputs.iid }}
        run: |
          set -euo pipefail
          RUN="$(date -u +%Y%m%dT%H%M%SZ)"
          echo "run=$RUN" >> "$GITHUB_OUTPUT"

          cat > remote.sh <<'EOS'
          #!/usr/bin/env bash
          set -euo pipefail
          export PATH="$HOME/.local/bin:$PATH"

          BUCKET="${BUCKET}"
          REGION="${REGION}"
          RUN="${RUN}"

          # --- Install dependencies (best-effort) ---
          sudo dnf -y install lynis openscap openscap-scanner scap-security-guide jq python3 || \
          sudo yum -y install lynis openscap openscap-scanner scap-security-guide jq python3 || true
          sudo dnf -y install ansible-core || sudo yum -y install ansible-core || python3 -m pip install --user --upgrade ansible

          # --- Helpers ---
          find_ds() {
            for p in /usr/share/xml/scap/ssg/ssg-al2023-ds.xml \
                     /usr/share/xml/scap/ssg/ssg-amazon_linux_2-ds.xml ; do
              test -f "$p" && { echo "$p"; return; }
            done
            find /usr/share/xml/scap/ssg -maxdepth 1 -type f -name 'ssg-*-ds.xml' -print -quit 2>/dev/null || true
          }

          run_scans() {
            stage="$1"; out="/tmp/reports/$stage"; mkdir -p "$out"

            # Lynis
            lynis audit system --nocolors --quick \
              --logfile "$out/lynis.log" \
              --report-file "$out/lynis-report.dat" || true

            # OpenSCAP if content present
            DS="$(find_ds || true)"
            if [ -n "${DS:-}" ] && [ -f "$DS" ]; then
              PROF="$(oscap info "$DS" | awk '/^ *Profile/{print $2}' | head -1 || true)"
              oscap xccdf eval --fetch-remote-resources \
                --results "$out/openscap-results.xml" \
                --report  "$out/openscap-report.html" \
                ${PROF:+--profile "$PROF"} \
                "$DS" || true
            else
              : > "$out/openscap-results.xml"
              echo "<!-- no SSG content on this AMI -->" > "$out/openscap-report.html"
            fi

            # Summarize -> summary.json
            python3 - "$out/lynis-report.dat" "$out/openscap-results.xml" "$out/summary.json" <<'PY'
import json, re, sys
def read(p):
    try: return open(p, encoding="utf-8", errors="ignore").read()
    except: return ""
def idx(t):
    m=re.search(r'hardening[_ ]index[^0-9]*([0-9]{1,3})', t, re.I)
    return int(m.group(1)) if m else 0
def score_xml(x):
    m=re.search(r'<score[^>]*>([0-9]+(?:\.[0-9]+)?)</score>', x, re.I)
    if m:
        v=float(m.group(1))
        return round(v*100,2) if 0<=v<=1 else round(v,2)
    return 0.0
lynis = read(sys.argv[1])
oscap = read(sys.argv[2])
data = {"lynis_hardening_index": idx(lynis), "openscap_score": score_xml(oscap)}
open(sys.argv[3], "w").write(json.dumps(data))
print(json.dumps(data))
PY

            # Upload stage artifacts
            for f in lynis-report.dat lynis.log openscap-report.html openscap-results.xml summary.json; do
              [ -f "$out/$f" ] && \
                aws s3 cp "$out/$f" "s3://$BUCKET/$RUN/demo/$stage/$f" --region "$REGION" || true
            done
          }

          # BEFORE pass
          run_scans before

          # Minimal hardening between passes (tiny, safe changes)
          if command -v ansible-playbook >/dev/null 2>&1; then
            cat >/tmp/minihardening.yml <<'YAML'
- hosts: localhost
  connection: local
  become: true
  gather_facts: false
  tasks:
    - name: Disable SSH root login
      lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#?\s*PermitRootLogin\s+.*'
        line: 'PermitRootLogin no'
        backup: yes
      notify: restart sshd
    - name: Ensure fail2ban present (best-effort)
      package:
        name: fail2ban
        state: present
      ignore_errors: yes
  handlers:
    - name: restart sshd
      service: { name: sshd, state: restarted }
YAML
            ansible-playbook -i "localhost," -c local --become /tmp/minihardening.yml || true
          fi

          # AFTER pass
          run_scans after

          # Keep a /latest/ copy
          aws s3 sync "s3://$BUCKET/$RUN/" "s3://$BUCKET/latest/" --delete --region "$REGION" || true

          # Emit both summaries for runner to capture
          echo '---SUMMARY-BEFORE---'; cat /tmp/reports/before/summary.json || true; echo '---END-BEFORE---'
          echo '---SUMMARY-AFTER---';  cat /tmp/reports/after/summary.json  || true; echo '---END-AFTER---'
          EOS

          chmod +x remote.sh
          KEY="ci-scripts/${RUN}/remote.sh"
          aws s3 cp remote.sh "s3://${BUCKET}/${KEY}" --region "${AWS_REGION}"
          echo "key=${KEY}" >> "$GITHUB_OUTPUT"

      - name: Send SSM command (download & execute remote script)
        id: ssm
        env:
          IID: ${{ steps.resolve.outputs.iid }}
          RUN: ${{ steps.stage.outputs.run }}
          KEY: ${{ steps.stage.outputs.key }}
        run: |
          set -euo pipefail
          CMD_ID=$(aws ssm send-command \
            --document-name "AWS-RunShellScript" \
            --instance-ids "$IID" \
            --comment "Compliance scan $RUN" \
            --parameters commands="aws s3 cp s3://$BUCKET/$KEY /tmp/remote.sh --region $AWS_REGION && sudo env BUCKET=$BUCKET REGION=$AWS_REGION RUN=$RUN bash /tmp/remote.sh" \
            --query "Command.CommandId" --output text)
          echo "cmd_id=$CMD_ID" >> "$GITHUB_OUTPUT"

          # Wait for completion
          STATUS=""
          for _ in $(seq 1 90); do
            STATUS=$(aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$IID" --query Status --output text || true)
            [[ "$STATUS" =~ ^(Success|Failed|Cancelled|TimedOut)$ ]] && break
            sleep 5
          done
          test "$STATUS" = "Success" || { echo "::error::SSM command ended with status: $STATUS"; exit 1; }

          # Pull stdout and extract the summaries
          aws ssm get-command-invocation --command-id "$CMD_ID" --instance-id "$IID" --output json > out.json
          jq -r '.StandardOutputContent' out.json > stdout.txt
          awk '/---SUMMARY-BEFORE---/{f=1;next}/---END-BEFORE---/{f=0}f' stdout.txt > summary-before.json || true
          awk '/---SUMMARY-AFTER---/{f=1;next}/---END-AFTER---/{f=0}f'   stdout.txt > summary-after.json  || true

      - name: Emit CloudWatch metrics (before & after; Run and latest)
        env:
          IID: ${{ steps.resolve.outputs.iid }}
          RUN: ${{ steps.stage.outputs.run }}
        run: |
          set -euo pipefail
          # Safe numeric coercion
          B_LHI=$(jq -r 'try (.lynis_hardening_index|tonumber) catch 0' summary-before.json 2>/dev/null || echo 0)
          A_LHI=$(jq -r 'try (.lynis_hardening_index|tonumber) catch 0' summary-after.json  2>/dev/null || echo 0)
          B_OSC=$(jq -r 'try (.openscap_score|tonumber)       catch 0' summary-before.json 2>/dev/null || echo 0)
          A_OSC=$(jq -r 'try (.openscap_score|tonumber)       catch 0' summary-after.json  2>/dev/null || echo 0)

          ts="$(date -u +%FT%TZ)"
          build_payload() {
            local run="$1"
            jq -n --arg iid "$IID" --arg run "$run" \
              --argjson bl "$B_LHI" --argjson al "$A_LHI" \
              --argjson bo "$B_OSC" --argjson ao "$A_OSC" \
              '[
                {"MetricName":"LynisHardeningIndex","Value":$bl,"Unit":"Count","Timestamp":"'"$ts"'",
                 "Dimensions":[{"Name":"InstanceId","Value":$iid},{"Name":"Run","Value":$run},{"Name":"Stage","Value":"before"}]},
                {"MetricName":"LynisHardeningIndex","Value":$al,"Unit":"Count","Timestamp":"'"$ts"'",
                 "Dimensions":[{"Name":"InstanceId","Value":$iid},{"Name":"Run","Value":$run},{"Name":"Stage","Value":"after"}]},
                {"MetricName":"OpenSCAPScore","Value":$bo,"Unit":"Percent","Timestamp":"'"$ts"'",
                 "Dimensions":[{"Name":"InstanceId","Value":$iid},{"Name":"Run","Value":$run},{"Name":"Stage","Value":"before"}]},
                {"MetricName":"OpenSCAPScore","Value":$ao,"Unit":"Percent","Timestamp":"'"$ts"'",
                 "Dimensions":[{"Name":"InstanceId","Value":$iid},{"Name":"Run","Value":$run},{"Name":"Stage","Value":"after"}]},
                {"MetricName":"Status","Value":1,"Unit":"Count","Timestamp":"'"$ts"'",
                 "Dimensions":[{"Name":"InstanceId","Value":$iid},{"Name":"Run","Value":$run}]}
              ]'
          }
          build_payload "$RUN"   > metrics-run.json
          build_payload "latest" > metrics-latest.json

          echo "=== Metrics (Run) ===";     jq . metrics-run.json
          echo "=== Metrics (latest) ===";  jq . metrics-latest.json

          aws cloudwatch put-metric-data --namespace "ComplianceMetrics" --metric-data file://metrics-run.json
          aws cloudwatch put-metric-data --namespace "ComplianceMetrics" --metric-data file://metrics-latest.json

      - name: Publish CloudWatch dashboard (best-effort)
        if: ${{ inputs.publish_dashboard }}
        env:
          REGION: ${{ inputs.region }}
          IID: ${{ steps.resolve.outputs.iid }}
        run: |
          set -euo pipefail
          if [ -f scripts/publish-cloudwatch-dashboard.sh ]; then
            bash scripts/publish-cloudwatch-dashboard.sh || true
          else
            echo "Dashboard script not found; skipping."
          fi

      - name: Upload diagnostics (artifact)
        uses: actions/upload-artifact@v4
        with:
          name: compliance-${{ steps.stage.outputs.run }}
          path: |
            summary-before.json
            summary-after.json
            metrics-run.json
            metrics-latest.json
            out.json
            stdout.txt

      - name: Output links
        run: |
          set -euo pipefail
          echo "Artifacts (S3 paths):"
          echo "  s3://${BUCKET}/${{ steps.stage.outputs.run }}/"
          echo "  s3://${BUCKET}/latest/"
          echo
          echo "If the bucket has static website hosting, try:"
          echo "  http://${BUCKET}.s3-website-${AWS_REGION}.amazonaws.com/${{ steps.stage.outputs.run }}/"
          echo "  http://${BUCKET}.s3-website-${AWS_REGION}.amazonaws.com/latest/"

